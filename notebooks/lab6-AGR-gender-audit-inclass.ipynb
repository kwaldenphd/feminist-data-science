{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGR Gender Language Audit ##\n",
    "\n",
    "*Lab based on and builds upon the research of Os Keyes in [this paper](https://ironholds.org/resources/papers/agr_paper.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will examine and (hopefully) build upon the work of Os Keyes in their paper, \"[Misgendering Machines: Trans/HCI Implicaitons of Automatic Gender Recognition](https://ironholds.org/resources/papers/agr_paper.pdf).\" \n",
    "\n",
    "Keyes's paper seeks to answer the following two research questions:\n",
    "\n",
    "1. How does Automated Gender Recognition research operationalise gender, and what are the possible consequences of this should it be widely deployed?\n",
    "2. How does HCI research interacting with AGR operationalise gender and contextualise any gendered assumptions of AGR software?\n",
    "\n",
    "They do so via content analysis (hand-coding the papers and then counting the results). \n",
    "\n",
    "Keyes was kind enough to share the metadata from the papers used to answer the first question with us, so that's what we'll be using as the basis of the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "First let's load the libraries we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure we have all the right libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load the metadata for the AGR paper analysis. Note that this is Os's own reserach data so please don't post this publicly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGR_metadata = pd.read_csv('AGR-metadata.csv', index_col=0)\n",
    "\n",
    "AGR_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that Keyes uses the following definitions:\n",
    "\n",
    "1. **binary**: Consisting of only two categories. \n",
    "2. **immutable**: Impossible to change once defined.  \n",
    "3. **physiological**: Rooted in external, biological features. \n",
    "4. **gender_focus**: Is the paper explicitly focused on developing AGR, or is it just using AGR to test a more general recognition algorithm?\n",
    "\n",
    "Note that something wonky is going on with the **datasets** column, and we'll need to fix that later if we want to use it. (We might not--let's see)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Replication \n",
    "\n",
    "To begin our analysis, let's replicate the counts that Keyes obtains:\n",
    "\n",
    "**Using the ``value_counts()`` function, show how many papers were published in each venue:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's see if we can replicate the binary/immutable/physiological percentages that Keyes finds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a smaller dataframe to work with, since we only care about a few of the categories.\n",
    "# We can use the \"filter() function to do some of the work for us.\n",
    "\n",
    "gender_analysis = AGR_metadata.filter([\"binary\",\"immutable\",\"physiology\",\"gender_focus\"], axis=1)\n",
    "\n",
    "gender_analysis.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we need to replace some of the values, since we are counting explicit and implicit mentions together.\n",
    "# We can use the replace() function for this. \n",
    "\n",
    "gender_analysis = gender_analysis.replace(to_replace = [\"implicit\", \"explicit\"], value=\"yes\")\n",
    "\n",
    "gender_analysis.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You do the same thing to replace the \"unmentioned\" with \"no\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we can just use crosstabs to generate our percentages\n",
    "\n",
    "pd.crosstab(gender_analysis[\"gender_focus\"], gender_analysis[\"binary\"], normalize=\"index\", margins=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the same for \"immutable\" and \"physiological\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why are these percentages a problem? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is fixing AGR the solution? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the argument for...\n",
    "\n",
    "#### Avoiding implementing AGR?\n",
    "#### Examining gender with inclusive methods? (e.g. self-disclosure vs. assignation)?\n",
    "#### Framing gender explicitly and with trans-inclusivitiy at the start of any research proejct?\n",
    "#### Making resources available for gender-aware HCI (and ML / data science)?\n",
    "#### Designing replacement methodologies?\n",
    "#### Digging deeper into AGR at level of datasets, codebases, perspectives of researchers..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's try to do a little bit of work towards that final aim. We're going to dig deeper into the language employed in the papers that Keyes studies. To do so, we're going to use some very basic text analysis techniques: namely, counting and sorting words. \n",
    "\n",
    "### Exploring the language of AGR reserach\n",
    "\n",
    "So let's get started. To begin, we need to read in the text of each of the papers, which I've assembled as a dataset for you. \n",
    "\n",
    "We'll store all of the papers as a list, ``all_papers``, with the text of each paper stored as a single item. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = \"./AGR-text/\" \n",
    "\n",
    "all_papers = [] # our list which will store the text of each doc; empty for now\n",
    "\n",
    "papers = sorted(os.listdir(base_dir)) # get a list of all the files in the directory\n",
    "\n",
    "for paper in papers: # iterate through the docs\n",
    "    if not paper.startswith('.'): # get only the .txt files\n",
    "        with open(base_dir + paper, \"r\", encoding=\"ISO-8859-1\") as file: # force format conversion to keep PCs happy\n",
    "            text = file.read() # read in the file as a single text string\n",
    "            all_papers.append(text) # append it to the all_docs list\n",
    "\n",
    "# lastly, just take a look at the last list item to be sure it worked\n",
    "\n",
    "all_papers[57]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we'll be using a library called [TextBlob](https://textblob.readthedocs.io/), which is a simplified text processing library that sits on top of [NLTK](http://www.nltk.org/). It works like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "all_papers_text = \"\"\n",
    "\n",
    "# make one giant string\n",
    "for paper in all_papers:\n",
    "    all_papers_text += paper\n",
    "\n",
    "# convert giant string into a single TextBlob object\n",
    "all_text = TextBlob(all_papers_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With TextBlob, counting words is very easy (if a little slow). You do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text.word_counts['male']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can you find the counts for the word \"female\"? What about \"men\" vs. \"women\"? \"Man\" vs. \"woman\"? Anything interesting in there?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing that's easy to do using TextBlob is to segment (or \"tokenize\") by sentence and word, as in this example, which prints out all of the sentences that contain the word \"woman\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in all_text.sentences:\n",
    "    for word in sentence.words:\n",
    "        if word == \"woman\":\n",
    "            print(str(sentence))\n",
    "            break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also calculate the sentiment score of any particular sentence, as well as a measure called \"subjectivity.\" Both of these scores are very rough approximations of the thing they purport to measure, and they're not very accurate in many contexts. But they can be fun to play around with. For example, here are the polarity and subjectivity scores for the \"woman\" sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in all_text.sentences:\n",
    "    for word in sentence.words:\n",
    "        if word == \"woman\":\n",
    "            print(\"A woman sentence! Polarity: \" + str(round(sentence.sentiment.polarity, 3)) + \". Subjectivity: \" + str(round(sentence.sentiment.subjectivity, 3)))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Are there ways that you can think of using word counts alongside the metadata we have about the AGR papers to see if we can learn anything more about how the researchers are framing their work?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One nice thing about the ``word_counts`` object is that it's actually a Python dictionary, so it can be sorted as so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import operator \n",
    "\n",
    "sorted_word_counts = OrderedDict(sorted(all_text.word_counts.items(), key=operator.itemgetter(1),reverse=True))\n",
    "\n",
    "top_10 = dict(list(sorted_word_counts.items())[0: 10])\n",
    "\n",
    "print(top_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a common problem in text analysis, which is that the top words are all of the most commonly used words, which means they are not very interesting for our purposes. \n",
    "\n",
    "One common approach to this problem is to filter the words by what are called \"stopwords\" -- a list of the most common words in a particular language or context. \n",
    "\n",
    "The code below filters `sorted_word_counts` and produces another OrderedDict, `filtered_wc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_wc = OrderedDict()\n",
    "\n",
    "for key, value in sorted_word_counts.items():\n",
    "    if key not in stop_words:\n",
    "        filtered_wc.update({key: value}) \n",
    "\n",
    "top_10 = dict(list(filtered_wc.items())[0: 10])\n",
    "\n",
    "print(top_10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These top words look a little better!\n",
    "\n",
    "Let's compare the top words employed in the papers with explicit vs. implicit vs. unstated gender binaries. In order to do that, we'll need to make three separate TextBlobs, one for each set of texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start w/ strings\n",
    "explicit = \"\"\n",
    "implicit = \"\"\n",
    "unmentioned = \"\"\n",
    "\n",
    "# go through each row of the metadata df and check the binary category;\n",
    "# depending on that category, add the corresponding text to the correct string\n",
    "for index, row in AGR_metadata.iterrows():\n",
    "    if row[\"binary\"] == \"explicit\":\n",
    "        explicit += all_papers[index]\n",
    "    elif row[\"binary\"] == \"implicit\":\n",
    "        implicit += all_papers[index]\n",
    "    elif row[\"binary\"] == \"unmentioned\":\n",
    "        unmentioned += all_papers[index]\n",
    "\n",
    "# now convert each giant string to a textblob\n",
    "explicit = TextBlob(explicit)\n",
    "implicit = TextBlob(implicit)\n",
    "unmentioned = TextBlob(unmentioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You do the rest from here...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What else might be interesting to look for, count, or compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
